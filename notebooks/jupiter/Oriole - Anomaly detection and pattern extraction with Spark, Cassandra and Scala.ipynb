{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection and pattern extraction with Spark, Cassandra and Scala\n",
    "\n",
    "Today, geo-located data is available in a number of domains, ranging from healthcare to financial markets, to social services. In all these domains, extracting patterns and detecting anomalies and novelties from data has very concrete business outcomes. \n",
    "\n",
    "Anomaly detection can be defined as the process of finding which samples in the given dataset do not follow the given patterns and behave as though they were produced by a different mechanism. From detection follows action. Depending on the domain and the use case, we define them as anomalies or novelties and these signals are the triggers for applications such as personalized marketing and fraud alerting and notification.\n",
    "\n",
    "As more data gets ingested/produced via digital services, itâ€™s key to perform this sort of analytics at scale. In the open source space, technologies such as Spark and Cassandra are definitely instrumental to implement and execute modern data pipelines at scale.\n",
    "\n",
    "This Oriole is divided in two parts: Firstly, I will show how to collect data from Cassandra and bring it up to Spark for further analysis. In the second part of this Oriole, I will explore a number of techniques for detecting anomalies based on three different techniques:\n",
    "\n",
    "  - Statistics and Histograms\n",
    "  - Process Mining and Graph Analytics\n",
    "  - Clustering for Geo-Located Data with DBSCAN\n",
    "\n",
    "For this analysis, we are going to use the Gowalla Dataset [1]. The Gowalla dataset consists of a table of events, registered by anonymized users. Each event registers a user checking into a geolocated venue at a specific timestamp. The dataset is available at https://snap.stanford.edu/data/loc-gowalla.html\n",
    "\n",
    "[1] E. Cho, S. A. Myers, J. Leskovec. Friendship and Mobility: Friendship and Mobility: User Movement in Location-Based Social Networks ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2011.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "This notebook is running scala code and interfaces to a Spark cluster using the [Apache Toree](https://toree.incubator.apache.org/) project. Furthermore, Spark reads the data from Cassandra tables. Spark interfaces to Cassandra via the [Cassandra-Spark connector](https://github.com/datastax/spark-cassandra-connector). \n",
    "\n",
    "At the time of compiling this notebook, Spark 1.6.1 and Cassandra 3.5 were used. Here below the command to install the Spark - Scala Kernel on Jupiter. More instructions on this topic are available on Apache Toree [website](https://toree.incubator.apache.org/) and [github pages](https://github.com/apache/incubator-toree).\n",
    "\n",
    "```\n",
    "sudo jupyter-toree install --spark_home=${SPARK_HOME} \n",
    "--spark_opts='--packages com.datastax.spark:spark-cassandra\n",
    "-connector_2.10:1.6.0,graphframes:graphframes:0.1.0-spark1.6 \n",
    "--conf spark.cassandra.connection.host=localhost --conf \n",
    "spark.executor.memory=4g --conf spark.driver.memory=4g'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6.1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Scala version\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//sql context\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val sqlContext  = new SQLContext(sc)\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// spark-cassandra connector\n",
    "import com.datastax.spark.connector._\n",
    "import com.datastax.spark.connector.cql._\n",
    "\n",
    "import org.apache.spark.sql.cassandra.CassandraSQLContext\n",
    "val cc = new CassandraSQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL queries in Cassandra\n",
    "\n",
    "Cassandra is exposed via a SQL context, so there is not need to learn a separate syntax as Spark will map the query to the available features of the underlying storage system. See below a simple query accessing the name and the id of venues from a cassandra table. Also remember that sql statements are _staged_ but not _executed_ until some actual [actions](http://spark.apache.org/docs/latest/programming-guide.html#actions) needs to be computed. Examples of actions are for instance, **count**(), **first**(), **collect**()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val venues   = cc.sql(\"select vid, name from lbsn.venues\").as(\"venues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30366"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1073929,Sputnik Gallery]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you cannot push to cassandra the full query, as the full query cannot be mapped on the available database functions supported on that specific data store. For instance, Cassandra cannot easily execute joins. In this case, Spark will partition and plan the query _pushing down_ what can be done in Cassandra and perform in Spark the rest of the query. \n",
    "\n",
    "More information can be found on Cassandra Documentation about [using Spark SQL to query data](http://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/spark/sparkSqlOverview.html) or on the [Cassandra Spark Connector](https://github.com/datastax/spark-cassandra-connector) pages.\n",
    "\n",
    "For example, the following query filters out only those events which were registered in the New York area. As filtering in cassandra cannot by done on columns which are not indexes, this specific query will first move the data form Cassandra to Spark, and then will perform the filtering in Spark. In general, it's a good practice to push down and filter as much data as early as possible. This practice keeps the throughput low and minimize the data transfered from one system to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val events   = cc.sql(\"\"\"select ts, uid, lat, lon, vid from lbsn.events where\n",
    "                            lon>-74.2589 and lon<-73.7004 and \n",
    "                            lat> 40.4774 and lat< 40.9176\n",
    "                      \"\"\").as(\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138948"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2009-12-17 14:58:23.0,83942,40.7586191119,-73.9888966084,225371]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into anomaly detection of geo-located data, let's perform some more basic queries. Herebelow, it is shown how to count events registered by user uid=0, and how to retrieve the name of venue id 7239827. Finally, the third query prints out the first five rows of the venue table, in no particular order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// User 0: how many checkins?\n",
    "events.where(\"uid=0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7239827,Central Park Manhattan ]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.where(\"vid = 7239827\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|vid    |name                |\n",
      "+-------+--------------------+\n",
      "|1073929|Sputnik Gallery     |\n",
      "|1350193|Kaori's Closet Tokyo|\n",
      "|1425555|Club Spain          |\n",
      "|7160165|La Quinta - Brooklyn|\n",
      "|285750 |La Sorrentina       |\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "venues.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining Cassandra tables in Spark.\n",
    "\n",
    "One of the advantages of connecting Cassandra and Spark, is the fact that you can now merge and join Cassandra tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+-------------+--------------+-------+--------------------+\n",
      "|ts                   |uid   |lat          |lon           |vid    |name                |\n",
      "+---------------------+------+-------------+--------------+-------+--------------------+\n",
      "|2009-12-17 14:58:23.0|83942 |40.7586191119|-73.9888966084|225371 |Smith's Bar & Grill |\n",
      "|2010-08-18 03:08:36.0|166583|40.697517133 |-74.175481333 |1276055|Jake's Coffeehouse  |\n",
      "|2010-09-14 14:15:22.0|41568 |40.7538465143|-73.9845085144|1308099|The Southwest Porch |\n",
      "|2009-12-04 16:24:03.0|74122 |40.721437025 |-73.9978015423|15998  |La Esquina          |\n",
      "|2009-12-04 16:23:01.0|74122 |40.7211204821|-73.9982306578|32674  |Blip.tv Headquarters|\n",
      "+---------------------+------+-------------+--------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val df_ny = events.\n",
    "  join(venues, events(\"events.vid\") === venues(\"venues.vid\"), \"inner\").\n",
    "  select(\"ts\", \"uid\", \"lat\", \"lon\", \"events.vid\",\"venues.name\")\n",
    "\n",
    "df_ny.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spark table joined above is going to be the starting point for our anomaly analysis.    \n",
    "Each row records the event's timestamp, the user id, the geo-location (latitude and longitude) of venue and finally the venue id and name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing SQL statements as code.\n",
    "\n",
    "Spark dataframes can also be filtered and transformed programmatically via a number of [pre-defined functions](https://spark.apache.org/docs/1.6.1/api/scala/#org.apache.spark.sql.functions$), such as min, sum, stddev, and many more. Some of those are shown in the next code sections. \n",
    "\n",
    "Next to the default set of pre-defined dataframe and column functions, it is possible to define the user-defined-functions (udf's). In the code below, we will create two UDF's to transform the timestamp to the day of the week and the hour of the day values, computed according to a given local timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// UDF functions for SQL-like operations on columns\n",
    "import org.joda.time.DateTime\n",
    "import org.joda.time.DateTimeZone\n",
    "\n",
    "import java.sql.Timestamp\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val  dayofweek = udf( (ts: Timestamp, tz: String) => {\n",
    "  val dt = new DateTime(ts,DateTimeZone.forID(tz))\n",
    "  // sunday starts at 0\n",
    "  dt.getDayOfWeek() - 1\n",
    "})\n",
    "\n",
    "val  localhour = udf( (ts: Timestamp, tz: String) => {\n",
    "  val dt = new DateTime(ts,DateTimeZone.forID(tz))\n",
    "  dt.getHourOfDay()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+-------------+--------------+-------+--------------------+---+----+\n",
      "|ts                   |uid   |lat          |lon           |vid    |name                |dow|hour|\n",
      "+---------------------+------+-------------+--------------+-------+--------------------+---+----+\n",
      "|2009-12-17 14:58:23.0|83942 |40.7586191119|-73.9888966084|225371 |Smith's Bar & Grill |3  |17  |\n",
      "|2010-08-18 03:08:36.0|166583|40.697517133 |-74.175481333 |1276055|Jake's Coffeehouse  |2  |6   |\n",
      "|2010-09-14 14:15:22.0|41568 |40.7538465143|-73.9845085144|1308099|The Southwest Porch |1  |17  |\n",
      "|2009-12-04 16:24:03.0|74122 |40.721437025 |-73.9978015423|15998  |La Esquina          |4  |19  |\n",
      "|2009-12-04 16:23:01.0|74122 |40.7211204821|-73.9982306578|32674  |Blip.tv Headquarters|4  |19  |\n",
      "+---------------------+------+-------------+--------------+-------+--------------------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val newyork_tz = \"America/New_York\"\n",
    "\n",
    "val df = df_ny.\n",
    "  withColumn(\"dow\",  dayofweek($\"ts\", lit(newyork_tz))).\n",
    "  withColumn(\"hour\", localhour($\"ts\", lit(newyork_tz))).\n",
    "  as(\"events\").cache()\n",
    "  \n",
    "df.show(5, false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "### Histogram based\n",
    "#### Basic statistics in Spark\n",
    "\n",
    "The following code section shows how to collect global statistics and histograms per hour of the day and per day of the week. Histograms can be made more specific by aggregating the events according to a number of factors, such as:\n",
    "\n",
    " - venue\n",
    " - geographical area\n",
    " - popular users\n",
    " - 1st, 2nd friend's circle\n",
    " \n",
    "If you are interested, in multiple slicing and dicing option, Spark as a [cube function](https://spark.apache.org/docs/1.5.1/api/scala/index.html#org.apache.spark.sql.DataFrame) as well.  \n",
    "To start, let's compute the histogram, accumulating all events and aggregating by hour of the day and by day of the week.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|dow|count|\n",
      "+---+-----+\n",
      "|  0|14640|\n",
      "|  1|15385|\n",
      "|  2|15686|\n",
      "|  3|16179|\n",
      "|  4|16558|\n",
      "|  5|18085|\n",
      "|  6|15849|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// histogram day of the week events\n",
    "df.groupBy($\"dow\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|count|\n",
      "+----+-----+\n",
      "|0   |2681 |\n",
      "|1   |1272 |\n",
      "|2   |852  |\n",
      "|3   |554  |\n",
      "|4   |323  |\n",
      "|5   |550  |\n",
      "|6   |937  |\n",
      "|7   |2157 |\n",
      "|8   |3615 |\n",
      "|9   |4586 |\n",
      "|10  |4903 |\n",
      "|11  |5576 |\n",
      "|12  |7257 |\n",
      "|13  |8142 |\n",
      "|14  |7687 |\n",
      "|15  |7254 |\n",
      "|16  |7519 |\n",
      "|17  |7495 |\n",
      "|18  |8621 |\n",
      "|19  |8970 |\n",
      "|20  |7167 |\n",
      "|21  |6127 |\n",
      "|22  |4747 |\n",
      "|23  |3390 |\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// histogram hour of the day events\n",
    "df.groupBy($\"hour\").count().show(24,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics in Spark: venue-specific histograms\n",
    "\n",
    "Moving on, let's have a look on how to create an specific histogram for each venue.   \n",
    "In this case, we will store the histogram as a vector. First, let's convert the day-of-the-week to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import breeze.linalg._\n",
    "import breeze.linalg.DenseVector\n",
    "\n",
    "import org.apache.spark.mllib.linalg.{Vector,Vectors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// vector histogram: the RDD way\n",
    "\n",
    "def toVector(i: Int, length:Int) = {\n",
    "  DenseVector((0 to length-1).map(x => if (x == i) 1.0 else 0.0).toArray)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225371,DenseVector(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select($\"vid\", $\"dow\").map(r => (r.getLong(0),toVector(r.getInt(1), 7))).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair RDDs: reduce by Key\n",
    "\n",
    "We will now **reduceByKey** those weekly and daily vectors by applying vectors arithmetics. In this way, we can collect the probability of an event happening at a specific day of the week for each venue in the dataset. This is a much more detailed analysis since different venues, such as restaurants, musea and train stations have different daily and weekly histogram patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val dow_hist = df.\n",
    "  select($\"vid\", $\"dow\").\n",
    "  map(r => (r.getLong(0),toVector(r.getInt(1), 7))).\n",
    "  reduceByKey(_ + _).\n",
    "  mapValues(x => Vectors.dense((x / sum(x)).toArray)).\n",
    "  toDF(\"vid\", \"dow_hist\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------+\n",
      "|vid    |dow_hist                       |\n",
      "+-------+-------------------------------+\n",
      "|713995 |[0.25,0.0,0.0,0.0,0.5,0.25,0.0]|\n",
      "|4415530|[0.0,0.0,0.0,0.0,0.0,1.0,0.0]  |\n",
      "|486265 |[0.25,0.0,0.0,0.0,0.0,0.5,0.25]|\n",
      "+-------+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dow_hist.show(3, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring events according to venues histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+-------------+--------------+-------+---+-------------------------------+\n",
      "|ts                   |uid   |lat          |lon           |vid    |dow|dow_hist                       |\n",
      "+---------------------+------+-------------+--------------+-------+---+-------------------------------+\n",
      "|2010-03-15 04:45:53.0|155999|40.7131656401|-74.0353098807|713995 |0  |[0.25,0.0,0.0,0.0,0.5,0.25,0.0]|\n",
      "|2010-06-19 13:08:21.0|41235 |40.7131656401|-74.0353098807|713995 |5  |[0.25,0.0,0.0,0.0,0.5,0.25,0.0]|\n",
      "|2010-04-23 20:47:34.0|10687 |40.7131656401|-74.0353098807|713995 |4  |[0.25,0.0,0.0,0.0,0.5,0.25,0.0]|\n",
      "|2010-10-08 04:53:51.0|74222 |40.7131656401|-74.0353098807|713995 |4  |[0.25,0.0,0.0,0.0,0.5,0.25,0.0]|\n",
      "|2010-09-25 10:02:38.0|181811|40.76603735  |-73.78952235  |4415530|5  |[0.0,0.0,0.0,0.0,0.0,1.0,0.0]  |\n",
      "+---------------------+------+-------------+--------------+-------+---+-------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val df_probs = df.\n",
    "  join(dow_hist, df(\"vid\") === dow_hist(\"vid\"), \"inner\").\n",
    "  select(\"ts\", \"uid\", \"lat\", \"lon\", \"events.vid\", \"dow\", \"dow_hist\").\n",
    "  cache()\n",
    "\n",
    "df_probs.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram based anomaly detection\n",
    "We will now score the probability of a given event based on the histograms we have just computed here above. For that, we are going to craft a new UDF which will select a given element of a vector based on the value provided by a different column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+------+---+--------+\n",
      "|ts                   |uid   |vid   |dow|dow_prob|\n",
      "+---------------------+------+------+---+--------+\n",
      "|2010-03-15 04:45:53.0|155999|713995|0  |0.25    |\n",
      "|2010-06-19 13:08:21.0|41235 |713995|5  |0.25    |\n",
      "|2010-04-23 20:47:34.0|10687 |713995|4  |0.5     |\n",
      "+---------------------+------+------+---+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val  nth = udf( (i:Int, arr: Vector) => {\n",
    "  arr.toArray.lift(i).getOrElse(0.0)\n",
    "})\n",
    "\n",
    "df_probs.select($\"ts\", $\"uid\", $\"vid\", $\"dow\", nth($\"dow\", $\"dow_hist\").as(\"dow_prob\")).show(3,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms anomaly detection: putting it all together\n",
    "\n",
    "Let's repeat the same exercise for the venue-specific histograms specifically for each venue. And thereafter, merge and compute the two probabilities for the day of the week or the hour of the day of a given event. Events with lower probabilities are less likely to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------------------------------------------------------+\n",
      "|vid    |hour_hist                                                                                          |\n",
      "+-------+---------------------------------------------------------------------------------------------------+\n",
      "|713995 |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0,0.0,0.0,0.0,0.0,0.0,0.25]|\n",
      "|4415530|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]  |\n",
      "|486265 |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.25,0.0,0.25,0.0,0.0,0.0,0.0]|\n",
      "+-------+---------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// same for hour of the day\n",
    "\n",
    "val hour_hist = df.\n",
    "  select($\"vid\", $\"hour\").\n",
    "  map(r => (r.getLong(0),toVector(r.getInt(1), 24))).\n",
    "  reduceByKey(_ + _).\n",
    "  mapValues(x => Vectors.dense((x / sum(x)).toArray)).\n",
    "  toDF(\"vid\", \"hour_hist\")\n",
    "\n",
    "hour_hist.show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+-------------+--------------+-----+--------------------+-------------------+\n",
      "|ts                   |uid   |lat          |lon           |vid  |hour_prob           |dow_prob           |\n",
      "+---------------------+------+-------------+--------------+-----+--------------------+-------------------+\n",
      "|2010-08-13 23:28:29.0|125327|40.7490532543|-73.9680397511|11831|0.015873015873015872|0.19047619047619047|\n",
      "|2010-09-29 12:08:36.0|578   |40.7490532543|-73.9680397511|11831|0.12698412698412698 |0.2222222222222222 |\n",
      "|2010-09-05 13:20:27.0|578   |40.7490532543|-73.9680397511|11831|0.047619047619047616|0.12698412698412698|\n",
      "|2010-07-21 07:02:07.0|578   |40.7490532543|-73.9680397511|11831|0.07936507936507936 |0.2222222222222222 |\n",
      "|2010-07-20 14:24:06.0|578   |40.7490532543|-73.9680397511|11831|0.07936507936507936 |0.14285714285714285|\n",
      "+---------------------+------+-------------+--------------+-----+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val df_probs = df.\n",
    "  join(dow_hist, df(\"vid\") === dow_hist(\"vid\"), \"inner\").\n",
    "  join(hour_hist, df(\"vid\") === hour_hist(\"vid\"), \"inner\").\n",
    "  select( \n",
    "    $\"ts\", \n",
    "    $\"uid\", \n",
    "    $\"lat\", \n",
    "    $\"lon\", \n",
    "    $\"events.vid\", \n",
    "    nth($\"hour\", $\"hour_hist\").as(\"hour_prob\"), \n",
    "    nth($\"dow\",  $\"dow_hist\").as(\"dow_prob\")).\n",
    "  cache()\n",
    "\n",
    "df_probs.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process mining based\n",
    "\n",
    "The first step, in order to do process mining, is to collect sequences of events. In particular, the following code will take chronologically consecuting events and bundle them in couples for a specific user. These tuples consists of two venue ids, namely source and destination venue id, defining where each user is coming from , going to respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// process mining\n",
    "val g_df = events.\n",
    "  select($\"ts\", $\"uid\", $\"vid\").\n",
    "  rdd.\n",
    "  map(row => (row.getLong(1), List( (row.getTimestamp(0), row.getLong(2)) ))).\n",
    "  reduceByKey(_ ++ _).\n",
    "  mapValues( x =>\n",
    "    x.sortWith(_._1.getTime < _._1.getTime).\n",
    "      map(_._2)\n",
    "  ).\n",
    "  mapValues(_.sliding(2).toList).\n",
    "  flatMap(_._2).\n",
    "  map(\n",
    "    _ match {\n",
    "      case List(a, b) => Some((a, b))\n",
    "      case _ => None\n",
    "  }).\n",
    "  flatMap(x => x).\n",
    "  toDF(\"src\", \"dst\").\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data frame is used to create a graph, where the nodes are the venues and the edges are the connection of users checking from the source venue to the destination venue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+------+------+\n",
      "|   src|   dst|\n",
      "+------+------+\n",
      "|255148|603177|\n",
      "|603177|603177|\n",
      "|603177|603177|\n",
      "|603177|603177|\n",
      "|603177|188022|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|   src|   dst|\n",
      "+------+------+\n",
      "|886164|487279|\n",
      "| 12149|748034|\n",
      "|104364| 15079|\n",
      "|248066|853505|\n",
      "|655145|985182|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val edges_df = g_df.\n",
    "  groupBy($\"src\",$\"dst\").\n",
    "  count().\n",
    "  select($\"src\",$\"dst\").\n",
    "  filter($\"src\" !== $\"dst\").\n",
    "  cache()\n",
    "\n",
    "edges_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|    id|\n",
      "+------+\n",
      "|220031|\n",
      "|105831|\n",
      "| 28031|\n",
      "| 11831|\n",
      "|434031|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val nodes_df = edges_df.\n",
    "  select($\"src\").\n",
    "  unionAll(edges_df.select($\"dst\")).\n",
    "  distinct().\n",
    "  toDF(\"id\").\n",
    "  cache()\n",
    "\n",
    "nodes_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes = 21429\n",
      "# edges = 108757\n"
     ]
    }
   ],
   "source": [
    "println(s\"# nodes = ${nodes_df.count()}\")\n",
    "println(s\"# edges = ${edges_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph and Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.graphframes.GraphFrame\n",
    "\n",
    "val g = GraphFrame(nodes_df, edges_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val results = g.pageRank.resetProbability(0.05).maxIter(5).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+---------------------------------+\n",
      "|vid   |pagerank          |name                             |\n",
      "+------+------------------+---------------------------------+\n",
      "|12505 |27.881643763277896|LGA LaGuardia Airport            |\n",
      "|23261 |26.385424367704715|JFK John F. Kennedy International|\n",
      "|11844 |22.11151212984308 |Times Square                     |\n",
      "|13022 |17.736644176952165|Grand Central Terminal           |\n",
      "|24963 |16.479439108529313|EWR Newark Liberty International |\n",
      "|11875 |10.202291677899952|Madison Square Garden            |\n",
      "|12525 |10.166984788526063|The Museum of Modern Art (MoMA)  |\n",
      "|11720 |10.0143847403669  |Yankee Stadium                   |\n",
      "|106840|9.76488986972341  |Union Square                     |\n",
      "|11834 |8.960647198372016 |Bryant Park                      |\n",
      "|12313 |8.15588197657451  |Empire State Building            |\n",
      "|14151 |6.999067399261288 |Rockefeller Center               |\n",
      "|17710 |6.986291334077661 |Madison Square Park              |\n",
      "+------+------------------+---------------------------------+\n",
      "only showing top 13 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val vertices = results.vertices.select(\"id\", \"pagerank\")\n",
    "val popular_venues = vertices.join(venues, vertices(\"id\") === venues(\"vid\"), \"inner\").select(\"vid\", \"pagerank\", \"name\")\n",
    "\n",
    "popular_venues.sort($\"pagerank\".desc).show(13, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo-Location: density based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from https://github.com/natalinobusa/nak/raw/master/nak_2.10-1.3.jar\n",
      "Finished download of nak_2.10-1.3.jar\n"
     ]
    }
   ],
   "source": [
    "%addjar https://github.com/natalinobusa/nak/raw/master/nak_2.10-1.3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val e_df = events.\n",
    "  select(\"uid\",\"lat\",\"lon\").\n",
    "  rdd.map(row => (row.getLong(0), Array(row.getDouble(1), row.getDouble(2))) ).\n",
    "  reduceByKey( _ ++ _).\n",
    "  mapValues(v => new DenseMatrix(v.length/2,2,v, 0, 2, true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def formatUserEvents(x: Tuple2[Long, DenseMatrix[Double]]) : Unit = {\n",
    "    val arr = x._2\n",
    "    val n = math.min( 5 , arr.rows) - 1\n",
    "    val slice = arr(0 to n, ::)\n",
    "    println(s\"uid = ${x._1}\")\n",
    "    println(s\"events count = ${arr.rows}\")\n",
    "    println(\"lat,lon = \")\n",
    "    println(slice)\n",
    "    if (arr.rows > 5) println(s\"... ${arr.rows- 5} more rows\")\n",
    "    println(\"-\"*30)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid = 33590\n",
      "events count = 355\n",
      "lat,lon = \n",
      "40.735486752   -73.979792352   \n",
      "40.73723724    -73.980824105   \n",
      "40.7424578176  -73.9837482386  \n",
      "40.70736247    -74.008827565   \n",
      "40.737524907   -73.978869923   \n",
      "... 350 more rows\n",
      "------------------------------\n",
      "uid = 139605\n",
      "events count = 4\n",
      "lat,lon = \n",
      "40.7126315333  -74.0444852833  \n",
      "40.6993813387  -74.0393972397  \n",
      "40.69856285    -74.03974055    \n",
      "40.6987988833  -74.0400195     \n",
      "------------------------------\n",
      "uid = 108050\n",
      "events count = 1\n",
      "lat,lon = \n",
      "40.7425115937  -74.0060305595  \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "e_df.take(3).foreach(e => formatUserEvents(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import breeze.numerics._\n",
    "import breeze.linalg._\n",
    "\n",
    "def euclideanDistance (a: DenseVector[Double], b: DenseVector[Double]) = norm(a-b, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nak.cluster._\n",
    "import nak.cluster.GDBSCAN._\n",
    "\n",
    "def dbscan(v : breeze.linalg.DenseMatrix[Double]) = {\n",
    "\n",
    "  val gdbscan = new GDBSCAN(\n",
    "    DBSCAN.getNeighbours(epsilon = 0.1, distance=euclideanDistance),\n",
    "    DBSCAN.isCorePoint(minPoints = 3)\n",
    "  )\n",
    "\n",
    "  val clusters = gdbscan cluster v\n",
    "  \n",
    "  // reducing the clusters to bounding boxes\n",
    "  // for simplicity: each user could \n",
    "  clusters.map(\n",
    "    cluster => (\n",
    "      cluster.id.toInt, \n",
    "      cluster.points.size, \n",
    "      cluster.points.map(_.value(0)).min,\n",
    "      cluster.points.map(_.value(1)).min,\n",
    "      cluster.points.map(_.value(0)).max,\n",
    "      cluster.points.map(_.value(1)).max\n",
    "    )\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val bboxRdd = e_df.mapValues(dbscan(_)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-------------+--------------+-------------+--------------+\n",
      "|  uid|cid|csize|      lat_min|       lon_min|      lat_max|       lon_max|\n",
      "+-----+---+-----+-------------+--------------+-------------+--------------+\n",
      "|33590| 66|    6| 40.837230222|  -73.88030725|40.8538124533| -73.782456383|\n",
      "|76360| 51|    2|40.8117304816|-74.0675711632|40.8117304816|-74.0675711632|\n",
      "| 4890|  2|   85|40.7008291704|-74.0347717925| 40.792863965|-73.9727404494|\n",
      "| 4890| 85|    3|40.6891968241|-74.0472292833|40.6993813387|-74.0393972397|\n",
      "|32640|  1|   12|40.6394147833|-74.0765082836|40.7058475346|  -73.98080408|\n",
      "|32640|  2|   12|40.7113103667|   -74.0023141|40.7317873406|  -73.94982115|\n",
      "| 6220|  1|   65|  40.70358305| -74.009206295|40.7636900022|-73.9719665051|\n",
      "| 6220| 56|    2|  40.85011295|-73.9520859718|40.8514833821|-73.9468287333|\n",
      "|10100|  1|   59|40.7190166905| -74.008749967|40.7805414319| -73.964475333|\n",
      "| 4605|  2|    4|40.7208271761|-73.9988958836|    40.752604|-73.9944867037|\n",
      "+-----+---+-----+-------------+--------------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val bbox_df = bboxRdd.\n",
    "  flatMapValues(x => x).\n",
    "  map(x => (x._1, x._2._1, x._2._2,x._2._3,x._2._4,x._2._5,x._2._6)).\n",
    "  toDF(\"uid\", \"cid\", \"csize\", \"lat_min\", \"lon_min\", \"lat_max\", \"lon_max\").\n",
    "  filter($\"cid\" > 0)\n",
    "\n",
    "bbox_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val bbox_events = events.\n",
    "  join(bbox_df, events(\"events.uid\") === bbox_df(\"uid\"), \"full\").\n",
    "  select(\"events.ts\",\"events.uid\",\"lat\",\"lon\",\"lat_min\",\"lon_min\",\"lat_max\",\"lon_max\").\n",
    "  filter($\"lat_min\".isNotNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class EventBbox(\n",
    "  ts: Timestamp,\n",
    "  uid: Long, \n",
    "  lat:Double, \n",
    "  lon: Double, \n",
    "  lat_min:Double, \n",
    "  lon_min:Double, \n",
    "  lat_max:Double, \n",
    "  lon_max:Double)\n",
    "  \n",
    "case class EventDetected(\n",
    "  ts: Timestamp,\n",
    "  uid: Long, \n",
    "  lat: Double, \n",
    "  lon: Double, \n",
    "  bbox: Boolean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bbox_check( x:EventBbox): Boolean = {\n",
    "  x.lon >= x.lon_min &\n",
    "  x.lon <= x.lon_max &\n",
    "  x.lat >= x.lat_min &\n",
    "  x.lat <= x.lat_max   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val scored_events = bbox_events.\n",
    "  as[EventBbox].\n",
    "  map(x => EventDetected(x.ts, x.uid, x.lat, x.lon, bbox_check(x))).\n",
    "  groupBy($\"ts\", $\"uid\").\n",
    "  reduce( (x,y) => if (x.bbox) x else y ).\n",
    "  map(x => x._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+--------------------+-----+-------------+--------------+-----+\n",
      "|                  ts|  uid|          lat|           lon| bbox|\n",
      "+--------------------+-----+-------------+--------------+-----+\n",
      "|2010-04-14 17:19:...|32640|   40.7263951|   -73.9917332| true|\n",
      "|2010-04-12 18:25:...|32640|40.6426269333|-74.0740269333| true|\n",
      "|2010-08-12 13:26:...|32640|40.6426269333|-74.0740269333| true|\n",
      "|2010-04-03 17:34:...|32640|40.7317873406|-73.9890950918| true|\n",
      "|2010-07-03 10:15:...|32640|40.6394147833|-74.0755963167| true|\n",
      "|2010-07-27 15:19:...|32640|   40.7036054|-74.0164375167| true|\n",
      "|2010-07-05 14:54:...|32640|  40.63023567|  -74.10869318|false|\n",
      "|2010-07-30 18:21:...|32640|40.7210690835|-73.9877271652| true|\n",
      "|2010-04-09 15:57:...|32640|40.7113103667|-73.9615914833| true|\n",
      "|2010-07-30 15:19:...|32640| 40.705748667| -74.003052617| true|\n",
      "+--------------------+-----+-------------+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scored_events.\n",
    "  filter(_.uid==32640).\n",
    "  show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
